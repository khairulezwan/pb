{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitbasecondabe15f5c191f44ef5875f848b4f50fd12",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from tensorflow.keras.application import VGG16\n",
    "from tensorflow.keras.application import imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyimagesearch.io import HDF5DatasetWriter\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import random \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'dataset' : '',\n",
    "    'output' : '',\n",
    "    'batch_size' : 32,\n",
    "    'buffer_size' : 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the batch size in a convenice var\n",
    "bs = args['batch_size']\n",
    "\n",
    "# grab the list of images that well be describing then randomly\n",
    "# shuffle the images to allow for easy training splits via\n",
    "# array slicing during training time\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = list(paths.list_images(args['dataset']))\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "# extract the class labels from the image paths then encode the \n",
    "# labels\n",
    "labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vgg16 network\n",
    "print(\"[INFO] loading VGG16 network..\")\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# init the HDF5 dataset write, then store the class label\n",
    "# names in the dataset\n",
    "dataset = HDF5DatasetWriter((len(imagePaths), 512 * 7 * 7), args['output'], dataKey='features', bufSize=args['buffer_size'])\n",
    "dataset.storeClassLabels(le.classLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the progress bar\n",
    "widgets = [\"Extracting features: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "pbar =  progressbar.ProgressBar(maxval=len(imagePaths), widgets=widgets).start()\n",
    "\n",
    "# loop over the image in batches    \n",
    "for i in np.arange(0, len(imagePaths), bs):\n",
    "    # extract the batch of image and labels, the init the\n",
    "    # list of actual images that will be passed through the net\n",
    "    # for feature extraction\n",
    "    batchPaths =  imagePaths[i:i +bs]\n",
    "    batchLabels = labels[i:i + bs]\n",
    "    batchImages = []\n",
    "\n",
    "    # loop over the image and labels in current batch\n",
    "    for (j, imagePaths) in enumerate(batchPaths):\n",
    "        # load the input image using the keras helper utility\n",
    "        # while ensuring the images is resized to 224 x 224\n",
    "        image = load_img(imagePaths, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "\n",
    "        # preprocess the image by (1) expanding the dimension\n",
    "        # and (2) subtracting the mean RGB pixel intensify from the\n",
    "        # imagenet dataset\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "        # add images to the batch\n",
    "        batchImages.append(image)\n",
    "\n",
    "    # pass the images through the network and use the output\n",
    "    # as our actual features\n",
    "    batchImages = np.vstack(batchImages)\n",
    "    features = model.predict(batchImages, batch_size=bs)\n",
    "\n",
    "    # reshape the features so that each image is represented by\n",
    "    # a flattended feature vector of the maxpooling2d outputs\n",
    "    features = features.reshape((features.shape[0], 512 * 7 * 7))\n",
    "\n",
    "    # add the features and labels to ou hdf5 datasets\n",
    "    dataset.add(features, batchLabels)\n",
    "    pbar.update(i)\n",
    "# close the dataset\n",
    "dataset.close()\n",
    "pbar.finish()"
   ]
  }
 ]
}